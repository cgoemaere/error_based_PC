{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory one level up (as if mnist_poc folder never existed)\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Define architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from lightning import seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "# Use proper initialization for Linear\n",
    "class MyLinear(nn.Linear):\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain(\"linear\")\n",
    "        # nn.init.xavier_uniform_(self.weight, gain)\n",
    "        nn.init.orthogonal_(self.weight, gain)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "\n",
    "architecture = (\n",
    "    [MyLinear(28 * 28, 128, bias=False)]\n",
    "    + [MyLinear(128, 128, bias=False) for _ in range(18)]\n",
    "    + [MyLinear(128, 10, bias=False)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Prerain architecture for a few steps\n",
    "Overall, this deep linear architecture is really not great. It has no output activation, which gives it bad results and great tendency for instability. That's why we only pretrain on a few batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodules import EMNIST\n",
    "from lightning import Trainer\n",
    "from pc_variants import BackpropMSE\n",
    "\n",
    "# 0: load dataset as Lightning DataModule\n",
    "datamodule = EMNIST(batch_size=64)\n",
    "print(\"Training on\", datamodule.dataset_name)\n",
    "\n",
    "# 1: Set up Lightning trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    logger=False,\n",
    "    max_epochs=2,\n",
    "    inference_mode=False,  # inference_mode would interfere with the state backward pass\n",
    "    limit_predict_batches=1,  # enable 1-batch prediction\n",
    ")\n",
    "\n",
    "# 2: Train model weights with backprop (fast & neutral method)\n",
    "pc = BackpropMSE(architecture, iters=None, e_lr=None, w_lr=0.001)\n",
    "trainer.fit(pc, datamodule=datamodule)\n",
    "trainer.test(pc, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Get a single batch x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "batch_size = 64\n",
    "dm = EMNIST(batch_size)\n",
    "print(\"Training on\", dm.dataset_name)\n",
    "dm.setup(\"fit\")\n",
    "dl = dm.train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "batch = dm.on_after_batch_transfer(batch, 0)\n",
    "x, y = batch[\"img\"], batch[\"y\"]\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Calculate analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analytical_solution import get_final_states\n",
    "\n",
    "true_optimum = get_final_states(architecture, x, y)\n",
    "for i, s_i in enumerate(true_optimum, start=1):\n",
    "    print(f\"x^{ {i} } shape: {s_i.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Do mini hyperparameter search to find best learning rate for states and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tracked_pce import TrackedPCE\n",
    "\n",
    "\n",
    "def get_conv_score(states):\n",
    "    result = sum(\n",
    "        (torch.norm(o - s, dim=1) / torch.norm(o, dim=1)).mean()\n",
    "        for o, s in zip(true_optimum, states[-1])\n",
    "    )\n",
    "    return result.item() / len(true_optimum)\n",
    "\n",
    "\n",
    "print(\"Starting hyperparameter sweep for Error Optimization...\")\n",
    "best_score = float(\"inf\")\n",
    "best_e_lr = None\n",
    "for e_lr in [0.01, 0.05, 0.1, 0.3]:\n",
    "    pc = TrackedPCE(architecture, iters=256, e_lr=e_lr, w_lr=0.001)\n",
    "    pc.minimize_error_energy(x, y)\n",
    "    score = get_conv_score(pc.log_errors)\n",
    "    print(e_lr, score)\n",
    "    if score <= best_score:\n",
    "        best_score = score\n",
    "        best_e_lr = e_lr\n",
    "\n",
    "print(\"Starting hyperparameter sweep for State Optimization...\")\n",
    "best_score = float(\"inf\")\n",
    "best_s_lr = None\n",
    "for s_lr in [0.1, 0.3, 0.5]:\n",
    "    pc.minimize_state_energy(x, y, iters=4096, s_lr=s_lr)\n",
    "    score = get_conv_score(pc.log_states)\n",
    "    print(s_lr, score)\n",
    "    if score <= best_score:\n",
    "        best_score = score\n",
    "        best_s_lr = s_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Rerun with optimal hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running Error Optimization with {best_e_lr=}\")\n",
    "pc = TrackedPCE(architecture, iters=256, e_lr=best_e_lr, w_lr=0.001)\n",
    "pc.minimize_error_energy(x, y)\n",
    "print(f\"Running State Optimization with {best_s_lr=}\")\n",
    "pc.minimize_state_energy(x, y, iters=4096, s_lr=best_s_lr)\n",
    "print(\"All done here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Make plot to compare activation convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_optimization_comparison(true_optimum, method1, method2, layers):\n",
    "    \"\"\"\n",
    "    Plot any number of layers side-by-side, comparing analytical optimum and two iterative methods.\n",
    "\n",
    "    Parameters:\n",
    "    - true_optimum: List of analytical solutions for each layer.\n",
    "    - method1: List of intermediate values for Error optim (indexed by time).\n",
    "    - method2: List of intermediate values for State optim (indexed by time).\n",
    "    - layers: List of integers specifying the layers to be plotted.\n",
    "    \"\"\"\n",
    "    if len(layers) == 0:\n",
    "        raise ValueError(\"You must specify at least one layer to plot.\")\n",
    "\n",
    "    # Create a figure with subplots for each layer\n",
    "    fig, axes = plt.subplots(1, len(layers), figsize=(4 * len(layers), 4), sharey=True)\n",
    "    axes = np.atleast_1d(axes)  # Ensure axes is always iterable, even for one layer\n",
    "\n",
    "    # Convert method1 and method2 lists into numpy arrays\n",
    "    method1 = [torch.stack(timestep[:-1]) for timestep in method1]\n",
    "    method2 = [torch.stack(timestep) for timestep in method2]\n",
    "\n",
    "    method1_array = np.array(method1)  # Shape: (time_steps_method1, states, batch_size, state_dim)\n",
    "    method2_array = np.array(method2)  # Shape: (time_steps_method2, states, batch_size, state_dim)\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Extract the true optimum for the current layer\n",
    "        optimum = np.array(true_optimum[layer].squeeze(0))  # Shape: (components,)\n",
    "\n",
    "        # Extract the corresponding components for method1 and method2\n",
    "        method1_layer_values = method1_array[:, layer, :, :]\n",
    "        method2_layer_values = method2_array[:, layer, :, :]\n",
    "\n",
    "        # Ensure method1_values have the same length as method2_values\n",
    "        if method1_layer_values.shape[0] < method2_layer_values.shape[0]:\n",
    "            method1_layer_values = np.concatenate(\n",
    "                [\n",
    "                    method1_layer_values,\n",
    "                    np.repeat(\n",
    "                        method1_layer_values[-1:],\n",
    "                        method2_layer_values.shape[0] - method1_layer_values.shape[0],\n",
    "                        axis=0,\n",
    "                    ),\n",
    "                ],\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "        # Take norm across state components\n",
    "        method1_layer_values = np.linalg.norm(\n",
    "            np.subtract(method1_layer_values, optimum, dtype=np.float64), ord=2, axis=-1\n",
    "        )\n",
    "        method2_layer_values = np.linalg.norm(\n",
    "            np.subtract(method2_layer_values, optimum, dtype=np.float64), ord=2, axis=-1\n",
    "        )\n",
    "\n",
    "        # Calculate medians and quartiles over the batch size\n",
    "        method1_q1 = np.percentile(method1_layer_values, 25, axis=-1)\n",
    "        method1_median = np.percentile(method1_layer_values, 50, axis=-1)\n",
    "        method1_q3 = np.percentile(method1_layer_values, 75, axis=-1)\n",
    "\n",
    "        method2_q1 = np.percentile(method2_layer_values, 25, axis=-1)\n",
    "        method2_median = np.percentile(method2_layer_values, 50, axis=-1)\n",
    "        method2_q3 = np.percentile(method2_layer_values, 75, axis=-1)\n",
    "\n",
    "        # Plot median and IQR\n",
    "        time_steps = np.arange(method1_median.shape[0])\n",
    "        ax.plot(time_steps, method2_median, \"C3-\", label=\"State Optimization\")\n",
    "        ax.fill_between(\n",
    "            time_steps,\n",
    "            method2_q1,\n",
    "            method2_q3,\n",
    "            color=\"C3\",\n",
    "            alpha=0.3,\n",
    "            label=\"IQR (State Optimization)\",\n",
    "        )\n",
    "        ax.plot(time_steps, method1_median, \"b-\", label=\"Error Optimization (ours)\")\n",
    "        ax.fill_between(\n",
    "            time_steps,\n",
    "            method1_q1,\n",
    "            method1_q3,\n",
    "            color=\"b\",\n",
    "            alpha=0.3,\n",
    "            label=\"IQR (Error Optimization)\",\n",
    "        )\n",
    "\n",
    "        # Add titles and labels\n",
    "        ax.set_title(f\"Convergence of $\\mathbf{{s_{{{layer}}}}}$ to optimum\", fontsize=15)\n",
    "        ax.set_xlabel(\"Update steps T (log scale)\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_ylim(7e-5, 1)\n",
    "\n",
    "        # Emphasize faster convergence\n",
    "        eo_converged = np.argmax(method1_median < 7e-5)\n",
    "        so_converged = np.argmax(method2_median < 7e-5)\n",
    "\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=(eo_converged, 9e-5),\n",
    "            xycoords=\"data\",\n",
    "            xytext=(so_converged, 9e-5),\n",
    "            textcoords=\"data\",\n",
    "            arrowprops=dict(arrowstyle=\"<->\", color=\"black\", lw=2),\n",
    "        )\n",
    "\n",
    "        # Caption above midpoint\n",
    "        how_much_faster = [\"140\", \"140\", \"130\"]\n",
    "        ax.text(\n",
    "            (eo_converged * so_converged) ** 0.5,\n",
    "            1e-4,\n",
    "            \"Same equilibrium,\\n~\" + how_much_faster[i] + \"× faster\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Set shared y-axis label\n",
    "    axes[0].set_ylabel(\"$\\|\\mathbf{s_i} - \\mathbf{s_i^*}\\|$\")\n",
    "\n",
    "    # Add legend\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], color=\"b\", linestyle=\"-\"),\n",
    "        plt.Line2D([0], [0], color=\"C3\", linestyle=\"-\"),\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        [\"Error-based PC (ours)\", \"State-based PC\"],\n",
    "        loc=\"upper center\",\n",
    "        ncol=2,\n",
    "        prop={\"size\": 12},\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])  # Make space for the global legend\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_comparison(\n",
    "    true_optimum,\n",
    "    pc.log_errors,\n",
    "    pc.log_states,\n",
    "    layers=[0, 9, 18],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Plot to compare gradient similarities between EO, SO and backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_states_to_grads(states_list):\n",
    "    result = []\n",
    "    pc.log_everything = False  # disable state tracking\n",
    "    for states in states_list:\n",
    "        pc.zero_grad()\n",
    "        E = pc.E_states_only(x, y, states) / batch_size\n",
    "        E.backward()\n",
    "        allgrads = [p.grad.clone().ravel() for p in pc.layers.parameters() if p.grad is not None]\n",
    "        result.append(allgrads)\n",
    "\n",
    "    # Reset everything back to how it was before...\n",
    "    pc.zero_grad()\n",
    "    pc.log_everything = True\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pc_grads = from_states_to_grads([true_optimum])  # shape: (1, layers, flat_param_len)\n",
    "eo_grads = from_states_to_grads(pc.log_errors)  # shape: (timesteps, layers, flat_param_len)\n",
    "so_grads = from_states_to_grads(pc.log_states)  # shape: (timesteps, layers, flat_param_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "def plot_gradient_comparison_BP(true_pc_grads, eo_grads, so_grads, layers):\n",
    "    \"\"\"\n",
    "    Plot cosine similarity of gradients wrt BP (the 1-step EO gradient).\n",
    "    Uses a broken y-axis to show both [0.0] and [0.9, 1.0].\n",
    "    A horizontal dashed line marks the analytical PC gradients similarity to BP.\n",
    "\n",
    "    Parameters:\n",
    "    - true_pc_grads: list of lists of tensors [timesteps][layers] with gradients of the analytical optimum\n",
    "    - eo_grads: list of lists of tensors [timesteps][layers] with gradients from Error Optimization\n",
    "    - so_grads: list of lists of tensors [timesteps][layers] with gradients from State Optimization\n",
    "    - layers: list of integers specifying the layers to be plotted\n",
    "    \"\"\"\n",
    "    if len(layers) == 0:\n",
    "        raise ValueError(\"You must specify at least one layer to plot.\")\n",
    "\n",
    "    # Create a figure with 2 rows (for broken axis) and len(layers) columns\n",
    "    fig, axes = plt.subplots(\n",
    "        2,\n",
    "        len(layers),\n",
    "        figsize=(4 * len(layers), 4),\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": [8, 1]},  # top panel taller\n",
    "    )\n",
    "    axes = np.atleast_2d(axes)  # shape (2, n_layers)\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        ax_top = axes[0, i]\n",
    "        ax_bottom = axes[1, i]\n",
    "\n",
    "        # Define BP = 1-step EO\n",
    "        bp_grads = eo_grads[1][layer]\n",
    "\n",
    "        # Extract trajectories\n",
    "        eo_layer_values = [step[layer] for step in eo_grads]\n",
    "        so_layer_values = [step[layer] for step in so_grads]\n",
    "\n",
    "        # Ensure same length by padding EO with its last value if needed\n",
    "        if len(eo_layer_values) < len(so_layer_values):\n",
    "            eo_layer_values += [eo_layer_values[-1]] * (len(so_layer_values) - len(eo_layer_values))\n",
    "\n",
    "        # Compute cosine similarity wrt bp_grads\n",
    "        eo_sims = [cosine_similarity(val, bp_grads, dim=0).cpu().numpy() for val in eo_layer_values]\n",
    "        so_sims = [cosine_similarity(val, bp_grads, dim=0).cpu().numpy() for val in so_layer_values]\n",
    "\n",
    "        eo_sims = np.array(eo_sims)\n",
    "        so_sims = np.array(so_sims)\n",
    "\n",
    "        # True optimum similarity wrt bp_grads (constant line)\n",
    "        optimum = true_pc_grads[0][layer].detach()\n",
    "        optimum_sim = cosine_similarity(optimum, bp_grads, dim=0).cpu().numpy().item()\n",
    "\n",
    "        # Plot curves on both axes\n",
    "        time_steps = np.arange(len(eo_sims))\n",
    "        for ax in (ax_top, ax_bottom):\n",
    "            ax.plot(time_steps, so_sims, \"C3-\", label=\"State-based PC\")\n",
    "            ax.plot(time_steps, eo_sims, \"b-\", label=\"Error-based PC (ours)\")\n",
    "            ax.axhline(optimum_sim, color=\"k\", linestyle=\"--\", label=\"Analytical PC gradients\")\n",
    "            ax.axhline(1.0, color=\"k\", linestyle=\":\", label=\"Backpropagation\")\n",
    "            ax.set_xscale(\"log\")\n",
    "\n",
    "        # Set y-limits for broken axis\n",
    "        ax_top.set_ylim(0.9, 1.012)\n",
    "        ax_bottom.set_ylim(-0.1, 0.1)\n",
    "        ax_bottom.set_yticks([0])\n",
    "        ax_bottom.set_yticklabels([\"0\"])\n",
    "\n",
    "        # Add diagonal break marks\n",
    "        d = 0.015\n",
    "        kwargs = dict(transform=ax_top.transAxes, color=\"k\", clip_on=False, linewidth=1)\n",
    "        ax_top.plot((-d, +d), (0, 0), **kwargs)  # left diagonal mark\n",
    "        ax_top.plot((1 - d, 1 + d), (0, 0), **kwargs)  # right diagonal mark\n",
    "\n",
    "        kwargs.update(transform=ax_bottom.transAxes)\n",
    "        ax_bottom.plot((-d, +d), (1, 1), **kwargs)\n",
    "        ax_bottom.plot((1 - d, 1 + d), (1, 1), **kwargs)\n",
    "\n",
    "        # Titles and labels\n",
    "        ax_top.set_title(f\"Gradient orientation at layer {layer}\", fontsize=15)\n",
    "        ax_bottom.set_xlabel(\"Update steps T (log scale)\")\n",
    "\n",
    "        # Hide spines between top and bottom\n",
    "        ax_top.spines[\"bottom\"].set_visible(False)\n",
    "        ax_bottom.spines[\"top\"].set_visible(False)\n",
    "        ax_top.xaxis.set_ticks_position(\"none\")  # hide bottom ticks of top axis\n",
    "        ax_bottom.tick_params(top=False)  # hide top ticks of bottom axis\n",
    "\n",
    "    # Shared y-axis label\n",
    "    axes[0, 0].set_ylabel(\"Cosine similarity w.r.t. backprop\")\n",
    "\n",
    "    # Legend\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], color=\"b\", linestyle=\"-\"),\n",
    "        plt.Line2D([0], [0], color=\"C3\", linestyle=\"-\"),\n",
    "        plt.Line2D([0], [0], color=\"k\", linestyle=\":\"),\n",
    "        plt.Line2D([0], [0], color=\"k\", linestyle=\"--\"),\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        [\n",
    "            \"Error-based PC (ours)\",\n",
    "            \"State-based PC\",\n",
    "            \"Backpropagation\",\n",
    "            \"Analytical PC gradients\",\n",
    "        ],\n",
    "        loc=\"upper center\",\n",
    "        ncol=4,\n",
    "        prop={\"size\": 12},\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_comparison_BP(\n",
    "    true_pc_grads,\n",
    "    eo_grads,\n",
    "    so_grads,\n",
    "    layers=[0, 9, 18],  # no biases, so the 18th param is simply the weight matrix of layer 18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_gradient_norms(true_pc_grads, eo_grads, so_grads, layers):\n",
    "    \"\"\"\n",
    "    Plot L2 norms of gradients over time for EO and SO.\n",
    "    Horizontal dashed lines mark bp_grads (2nd EO gradient) and analytical PC gradients.\n",
    "\n",
    "    Parameters:\n",
    "    - true_pc_grads: list of lists of tensors [timesteps][layers] with gradients of the analytical optimum\n",
    "    - eo_grads: list of lists of tensors [timesteps][layers] with gradients from Error Optimization\n",
    "    - so_grads: list of lists of tensors [timesteps][layers] with gradients from State Optimization\n",
    "    - layers: list of integers specifying the layers to be plotted\n",
    "    \"\"\"\n",
    "    if len(layers) == 0:\n",
    "        raise ValueError(\"You must specify at least one layer to plot.\")\n",
    "\n",
    "    # Create a figure with subplots for each layer\n",
    "    fig, axes = plt.subplots(1, len(layers), figsize=(4 * len(layers), 4), sharey=True)\n",
    "    axes = np.atleast_1d(axes)\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Define bp_grads = 1-step EO gradient\n",
    "        bp_grads = eo_grads[1][layer]\n",
    "\n",
    "        # Extract trajectories\n",
    "        eo_layer_values = [step[layer] for step in eo_grads]\n",
    "        so_layer_values = [step[layer] for step in so_grads]\n",
    "\n",
    "        # Ensure same length by padding EO with its last value if needed\n",
    "        if len(eo_layer_values) < len(so_layer_values):\n",
    "            eo_layer_values += [eo_layer_values[-1]] * (len(so_layer_values) - len(eo_layer_values))\n",
    "\n",
    "        # Compute L2 norms\n",
    "        eo_norms = [torch.norm(val, p=2).cpu().numpy() for val in eo_layer_values]\n",
    "        so_norms = [torch.norm(val, p=2).cpu().numpy() for val in so_layer_values]\n",
    "\n",
    "        eo_norms = np.array(eo_norms)\n",
    "        so_norms = np.array(so_norms)\n",
    "\n",
    "        # Reference norms (constant horizontal lines)\n",
    "        bp_norm = torch.norm(bp_grads, p=2).cpu().numpy().item()\n",
    "        optimum_norm = torch.norm(true_pc_grads[0][layer], p=2).cpu().numpy().item()\n",
    "\n",
    "        # Plot curves\n",
    "        time_steps = np.arange(len(eo_norms))\n",
    "        ax.plot(time_steps, so_norms, \"C3-\", label=\"State-based PC\")\n",
    "        ax.plot(time_steps, eo_norms, \"b-\", label=\"Error-based PC (ours)\")\n",
    "\n",
    "        # Plot reference lines\n",
    "        ax.axhline(bp_norm, color=\"k\", linestyle=\":\", label=\"Backpropagation\")\n",
    "        ax.axhline(optimum_norm, color=\"k\", linestyle=\"--\", label=\"Analytical PC gradients\")\n",
    "\n",
    "        # Titles and scaling\n",
    "        ax.set_title(f\"Gradient norm at layer {layer}\", fontsize=15)\n",
    "        ax.set_xlabel(\"Update steps T (log scale)\")\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "    # Shared y-axis label\n",
    "    axes[0].set_ylabel(\"‖grad‖₂\")\n",
    "\n",
    "    # Legend (no need for; will use the same legend as above)\n",
    "    # handles = [\n",
    "    #     plt.Line2D([0], [0], color=\"b\", linestyle=\"-\"),\n",
    "    #     plt.Line2D([0], [0], color=\"C3\", linestyle=\"-\"),\n",
    "    #     plt.Line2D([0], [0], color=\"k\", linestyle=\":\"),\n",
    "    #     plt.Line2D([0], [0], color=\"k\", linestyle=\"--\"),\n",
    "    # ]\n",
    "    # fig.legend(\n",
    "    #     handles,\n",
    "    #     [\n",
    "    #         \"Error Optimization (ours)\",\n",
    "    #         \"State Optimization\",\n",
    "    #         \"Backpropagation\",\n",
    "    #         \"Analytical PC gradients\",\n",
    "    #     ],\n",
    "    #     loc=\"upper center\",\n",
    "    #     ncol=4,\n",
    "    #     prop={\"size\": 12},\n",
    "    # )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_norms(\n",
    "    true_pc_grads,\n",
    "    eo_grads,\n",
    "    so_grads,\n",
    "    layers=[0, 9, 18],  # no biases, so the 18th param is simply the weight matrix of layer 18\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
